{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group classification with pretrained networks\n",
    "Let's first see how well the models can differentiate between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pretrained network\n",
    "\n",
    "inception = InceptionV3(weights='imagenet')\n",
    "model = Model(inception.input, inception.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189/189 [00:44<00:00,  4.29it/s]\n",
      "100%|██████████| 54/54 [00:12<00:00,  4.47it/s]\n",
      "100%|██████████| 8/8 [00:01<00:00,  4.46it/s]\n",
      "100%|██████████| 442/442 [01:42<00:00,  4.31it/s]\n",
      "100%|██████████| 268/268 [01:03<00:00,  4.22it/s]\n",
      "100%|██████████| 391/391 [01:30<00:00,  4.31it/s]\n",
      "0it [00:00, ?it/s]\n",
      "100%|██████████| 47/47 [00:10<00:00,  4.34it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  4.23it/s]\n",
      "100%|██████████| 371/371 [01:26<00:00,  4.30it/s]\n",
      "100%|██████████| 452/452 [01:43<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load images\n",
    "\n",
    "def load_images(image_folder):\n",
    "    image_features, labels = [], []\n",
    "    for idx, f in enumerate(os.listdir(image_folder)):\n",
    "        for path in tqdm(glob(image_folder+'/'+f+'/*.jpg')):\n",
    "            # Load image\n",
    "            image = load_img(path, target_size=(299, 299))\n",
    "            # Convert to array\n",
    "            image = img_to_array(image)\n",
    "            # Expand dims\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            # Preprocess image\n",
    "            image = preprocess_input(image)\n",
    "            # Generate features\n",
    "            features = model(image)\n",
    "            # Update lists\n",
    "            image_features.append(features.numpy())\n",
    "            labels.append(idx)\n",
    "            \n",
    "    return np.array(image_features), np.array(labels)        \n",
    "    \n",
    "            \n",
    "# Call the function\n",
    "image_features, labels = load_images('../../data')\n",
    "image_features = image_features.reshape(-1, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features and labels \n",
    "\n",
    "with open('../../saved_data/image_features.pkl', 'wb') as f:\n",
    "    pickle.dump(image_features, f)\n",
    "    \n",
    "with open('../../saved_data/labels.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier model\n",
    "\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "InputLayer = Input(shape=(2048,))\n",
    "DenseLayer = Dense(num_classes+1, activation='softmax')\n",
    "outputs = DenseLayer(InputLayer)\n",
    "\n",
    "classifier = Model(InputLayer, outputs)\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1149 - accuracy: 0.9792 - val_loss: 3.5135 - val_accuracy: 0.0966\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1110 - accuracy: 0.9820 - val_loss: 3.2437 - val_accuracy: 0.1213\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1075 - accuracy: 0.9831 - val_loss: 3.6413 - val_accuracy: 0.0921\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0992 - accuracy: 0.9843 - val_loss: 3.7785 - val_accuracy: 0.0854\n",
      "Epoch 5/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0949 - accuracy: 0.9859 - val_loss: 4.0178 - val_accuracy: 0.0764\n",
      "Epoch 6/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9859 - val_loss: 4.0795 - val_accuracy: 0.0742\n",
      "Epoch 7/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0895 - accuracy: 0.9904 - val_loss: 3.6662 - val_accuracy: 0.1011\n",
      "Epoch 8/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9865 - val_loss: 3.9093 - val_accuracy: 0.0831\n",
      "Epoch 9/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0789 - accuracy: 0.9899 - val_loss: 3.8857 - val_accuracy: 0.0854\n",
      "Epoch 10/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9910 - val_loss: 3.7927 - val_accuracy: 0.1056\n",
      "Epoch 11/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0756 - accuracy: 0.9916 - val_loss: 4.1144 - val_accuracy: 0.0787\n",
      "Epoch 12/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0722 - accuracy: 0.9904 - val_loss: 4.0618 - val_accuracy: 0.0831\n",
      "Epoch 13/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9927 - val_loss: 3.7487 - val_accuracy: 0.1124\n",
      "Epoch 14/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0661 - accuracy: 0.9938 - val_loss: 3.9715 - val_accuracy: 0.0921\n",
      "Epoch 15/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0641 - accuracy: 0.9949 - val_loss: 3.9548 - val_accuracy: 0.1034\n",
      "Epoch 16/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9933 - val_loss: 4.1734 - val_accuracy: 0.0831\n",
      "Epoch 17/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0564 - accuracy: 0.9961 - val_loss: 4.3253 - val_accuracy: 0.0809\n",
      "Epoch 18/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0540 - accuracy: 0.9966 - val_loss: 4.2011 - val_accuracy: 0.0854\n",
      "Epoch 19/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9961 - val_loss: 4.3739 - val_accuracy: 0.0831\n",
      "Epoch 20/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9978 - val_loss: 3.9880 - val_accuracy: 0.1101\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier \n",
    "\n",
    "hist = classifier.fit(image_features, to_categorical(labels), epochs=20, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
