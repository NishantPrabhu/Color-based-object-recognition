{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT + CNN features for image matching\n",
    "SIFT seems to be narrowing down on the correct set really well, especially with Flann based KNN matching. Once we receive the smaller set, we can use CNN features to find the best match I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from ntpath import basename\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception model to create features\n",
    "inception_model = InceptionV3(weights='imagenet')\n",
    "inception_model = Model(inception_model.inputs, inception_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN features and SIFT descriptors created earlier\n",
    "with open('../../saved_data/22 Jun/feature_stack.pkl', 'rb') as f:\n",
    "    feature_stack = pickle.load(f)\n",
    "    \n",
    "with open('../../saved_data/22 Jun/file_feature_map.pkl', 'rb') as f:\n",
    "    file_feature_map = pickle.load(f)\n",
    "    \n",
    "with open('../../saved_data/22 Jun/file_sift_map.pkl', 'rb') as f:\n",
    "    file_sift_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model performance\n",
    "\n",
    "class ObjectMatcher(object):\n",
    "    \n",
    "    def __init__(self, file_feature_map, file_sift_map):\n",
    "        self.file_feature_map = file_feature_map\n",
    "        self.file_sift_map = file_sift_map\n",
    "        self.paths = list(file_sift_map.keys())\n",
    "        \n",
    "        # Sift object and Flann matcher\n",
    "        FLANN_INDEX_KDTREE = 0\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        \n",
    "        self.sift_ = cv2.xfeatures2d.SIFT_create()\n",
    "        self.flann_ = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        \n",
    "        \n",
    "    def find_candidates(self, img_path, top_k=5):\n",
    "        \"\"\" Find candidates with SIFT features \"\"\"\n",
    "        # Dictionary to store good matches\n",
    "        file_matches_dict = {}\n",
    "        # Create sift features for input\n",
    "        img = cv2.imread(img_path, 0)\n",
    "        _, img_desc = self.sift_.detectAndCompute(img, None)\n",
    "        \n",
    "        for path in tqdm(self.paths):\n",
    "            desc = self.file_sift_map[path]\n",
    "            matches = self.flann_.knnMatch(desc, img_desc, k=2)\n",
    "            good = 0\n",
    "            for m, n in matches:\n",
    "                if m.distance < 0.7 * n.distance:\n",
    "                    good += 1\n",
    "            file_matches_dict.update({path: good})\n",
    "            \n",
    "        # Select top 5 in all matches\n",
    "        clear_output()\n",
    "        sorted_paths = sorted(self.paths, key=lambda x: file_matches_dict[x])\n",
    "        return sorted_paths[:top_k]\n",
    "    \n",
    "    \n",
    "    def get_features(self, model, img_path):\n",
    "        \"\"\" Returns features extracted by CNN model \"\"\"\n",
    "        \n",
    "        img = load_img(img_path, target_size=(299, 299))\n",
    "        img = img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        x = preprocess_input(img)\n",
    "        features = model(x)\n",
    "        return features.numpy()\n",
    "    \n",
    "    \n",
    "    def find_best_match(self, img_features, candidate_paths):\n",
    "        \"\"\" Uses CNN features to narrow down on best match \"\"\"\n",
    "        \n",
    "        feature_dict = {f: self.file_feature_map[f] for f in candidate_paths}\n",
    "        matcher = lambda x: np.linalg.norm(img_features - self.file_feature_map[x])\n",
    "        ranked_paths = sorted(candidate_paths, key=matcher)\n",
    "        best_match = ranked_paths[0]\n",
    "        best_3_match = ranked_paths[:3]\n",
    "        return best_match, best_3_match\n",
    "    \n",
    "    \n",
    "    def evaluate_model(self, root_dir):\n",
    "        \"\"\" Finds the accuracy of the model \"\"\"\n",
    "        \n",
    "        correct, correct3 = 0, 0\n",
    "        folders = os.listdir(root_dir)\n",
    "        folders.remove('test_images')\n",
    "        num_files = 0\n",
    "        \n",
    "        for fol in folders:\n",
    "            for path in tqdm(glob(root_dir+'/'+fol+'/*.jpg')):\n",
    "                num_files += 1\n",
    "                cnd_paths = self.find_candidates(path)\n",
    "                features = self.get_features(inception_model, path)\n",
    "                best, best3 = self.find_best_match(features, cnd_paths)\n",
    "                if basename(best) == basename(path):\n",
    "                    correct += 1\n",
    "                if basename(path) in [basename(p) for p in best3]:\n",
    "                    correct3 += 1\n",
    "                    \n",
    "        accuracy = 100. * correct / num_files\n",
    "        accuracy3 = 100. * correct3 / num_files\n",
    "        \n",
    "        # Accuracy\n",
    "        clear_output()\n",
    "        print(\"Accuracy: {:.2f}\".format(accuracy))\n",
    "        print(\"Accuracy in top 3: {:.2f}\".format(accuracy3))\n",
    "        \n",
    "    \n",
    "    def test_model(self, img_paths):\n",
    "        \"\"\" Tests the model for a few images \"\"\"\n",
    "        \n",
    "        for img_path in tqdm(img_paths):\n",
    "            cnd_paths = self.find_candidates(img_path)\n",
    "            features = self.get_features(inception_model, img_path)\n",
    "            _, best3 = self.find_best_match(features, cnd_paths)\n",
    "            best3_imgs = [cv2.imread(path, 0) for path in best3]\n",
    "\n",
    "            # Show image and best matches\n",
    "            test_img = cv2.imread(img_path, 0)\n",
    "            plt.imshow(test_img, cmap='gray')\n",
    "            plt.show()\n",
    "            fig = plt.figure()\n",
    "            for i in range(3):\n",
    "                fig.add_subplot(1, 3, i+1)\n",
    "                plt.imshow(best3_imgs[i], cmap='gray')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            print(\"\\n------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475d8bb39e174fe7b5bc7001b1f442a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f1dcf8447344c6b5779d6132d208ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2224), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9781625a7192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mobj_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_feature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_sift_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mobj_match\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-756bc0dff460>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(self, img_paths)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mcnd_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minception_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnd_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-756bc0dff460>\u001b[0m in \u001b[0;36mfind_candidates\u001b[0;34m(self, img_path, top_k)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_sift_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflann_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknnMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mgood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Choose some paths for testing\n",
    "paths = list(file_feature_map.keys())\n",
    "idx = np.random.randint(0, len(paths)-1, size=3)\n",
    "chosen_paths = [paths[i] for i in idx]\n",
    "\n",
    "# Testing \n",
    "obj_match = ObjectMatcher(file_feature_map, file_sift_map)\n",
    "obj_match.test_model(chosen_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
