{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Color quantization variant 2**\n",
    "Attempt to generate a unique key based on colors and their percentages in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "\n",
    "class ColorBar(object):\n",
    "    \"\"\"\n",
    "    Generate colorbar with prominent colors of input image\n",
    "    \n",
    "    Args:\n",
    "        size       :    Size of colorbar in pixels\n",
    "        n_colors   :    Number of prominent colors to extract\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=100, n_colors=10):\n",
    "        assert isinstance(size, int)\n",
    "        assert isinstance(n_colors, int)\n",
    "        \n",
    "        self.size = size\n",
    "        self.n_colors = n_colors\n",
    "        \n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        # MiniBatchKMeans for color quantization\n",
    "        img = np.array(sample)\n",
    "        img = img.reshape(img.shape[0]*img.shape[1], -1)\n",
    "        clf = MiniBatchKMeans(n_clusters=self.n_colors)\n",
    "        clf.fit(img)\n",
    "        centers, labels = clf.cluster_centers_, clf.labels_\n",
    "        \n",
    "        # Color histogram\n",
    "        # Delete the color with max pixels (belongs to white)\n",
    "        counts = Counter(labels)\n",
    "        max_value_id = np.argmax(counts.values())\n",
    "        max_label_locations = np.where(labels == list(counts.keys())[max_value_id])[0]\n",
    "        labels = np.delete(labels, max_label_locations)\n",
    "        centers = np.array(list(centers[:max_value_id]) + list(centers[max_value_id+1:]))\n",
    "        label_ids = np.delete(np.array(list(counts.keys())), max_value_id)\n",
    "        \n",
    "        num_labels = np.arange(0, len(label_ids)+1)\n",
    "        (hist, _) = np.histogram(labels, bins=num_labels)\n",
    "        hist = hist / sum(hist)\n",
    "        \n",
    "        # Create colorbar\n",
    "        bar = np.zeros((1, self.size, 3), dtype=np.uint8)\n",
    "        startX = 0\n",
    "\n",
    "        for (percent, color) in zip(hist, centers):\n",
    "            endX = startX + percent * self.size\n",
    "            cv2.rectangle(bar, (int(startX), 0), (int(endX), 50), color.astype('uint8').tolist(), -1)\n",
    "            startX = endX\n",
    "            \n",
    "        return bar\n",
    "    \n",
    "    \n",
    "# Define image transforms\n",
    "im_trans = transforms.Compose([\n",
    "    transforms.Resize(size=256),\n",
    "    ColorBar(size=100, n_colors=10),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root='../../images/train', transform=im_trans),\n",
    "    'valid': datasets.ImageFolder(root='../../images/validation', transform=im_trans),\n",
    "    'test': datasets.ImageFolder(root='../../images/test', transform=im_trans)\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(data['train'], batch_size=BATCH_SIZE, shuffle=True),\n",
    "    'valid': DataLoader(data['valid'], batch_size=BATCH_SIZE, shuffle=True),\n",
    "    'test': DataLoader(data['test'], batch_size=BATCH_SIZE, shuffle=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "\n",
    "N_CLASSES = 168\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=(1, 3), stride=1, bias=True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(98, N_CLASSES),\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "\n",
    "# Train and test functions\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_function, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    batch_loss = []\n",
    "    top_1_correct = 0\n",
    "    top_5_correct = 0\n",
    "    \n",
    "    for batch_id, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        top_1_preds = output.argmax(dim=1, keepdim=True)\n",
    "        top_5_preds = output.topk(5, dim=1)[1]\n",
    "        \n",
    "        batch_loss.append(loss.mean().item())\n",
    "        top_1_correct += top_1_preds.eq(target.view_as(top_1_preds)).sum().item()\n",
    "        top_5_correct += sum([1 if target[i] in top_5_preds[i] else 0 for i in range(len(target))])\n",
    "        \n",
    "        print(\"Epoch {} [Batch {}/{}] \\t Top 1 accuracy: {:.2f}% \\t Top 5 accuracy: {:.2f}%\".format(\n",
    "            epoch, batch_id+1, len(train_loader), \n",
    "            top_1_correct/(BATCH_SIZE * (batch_id+1)) * 100.,\n",
    "            top_5_correct/(BATCH_SIZE * (batch_id+1)) * 100.\n",
    "        ))\n",
    "        \n",
    "    train_loss = np.mean(batch_loss)\n",
    "    top_1_accuracy = top_1_correct / len(train_loader.dataset)\n",
    "    top_5_accuracy = top_5_correct / len(train_loader.dataset)\n",
    "    \n",
    "    return train_loss, top_1_accuracy, top_5_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def validate(model, val_loader, loss_function, epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    batch_loss = []\n",
    "    top_1_correct = 0\n",
    "    top_5_correct = 0\n",
    "    \n",
    "    for data, target in val_loader:\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        top_1_preds = output.argmax(dim=1, keepdim=True)\n",
    "        top_5_preds = output.topk(5, dim=1)[1]\n",
    "        \n",
    "        epoch_loss.append(loss.mean().item())\n",
    "        top_1_correct += top_1_preds.eq(target.view_as(top_1_preds)).sum().item()\n",
    "        top_5_correct += sum([1 if target[i] in top_5_preds[i] else 0 for i in range(len(target))])\n",
    "        \n",
    "    val_loss = np.mean(batch_loss)\n",
    "    top_1_accuracy = top_1_correct / len(val_loader.dataset)\n",
    "    top_5_accuracy = top_5_correct / len(val_laoder.dataset)\n",
    "    \n",
    "    print(\"\\n[VALIDATION] Epoch {} \\t Top 1 accuracy: {:.2f}% \\t Top 5 accuracy: {:.2f}%\".format(\n",
    "        epoch, top_1_accuracy*100., top_5_accuracy*100.\n",
    "    ))\n",
    "    print('\\n--------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    return val_loss, top_1_accuracy, top_5_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_loader, loss_function):\n",
    "    \n",
    "    model.eval()\n",
    "    top_1_correct, top_5_correct = 0, 0\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        top_1_preds = output.argmax(dim=1, keepdim=True)\n",
    "        top_5_preds = output.topk(5, dim=1)[1]\n",
    "        \n",
    "        top_1_correct += top_1_preds.eq(target.view_as(top_1_preds)).sum().item()\n",
    "        top_5_correct += sum([1 if target[i] in top_5_preds[i] else 0 for i in range(len(target))])\n",
    "        \n",
    "    top_1_accuracy = top_1_correct / len(test_loader.dataset)\n",
    "    top_5_accuracy = top_5_correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(\"[TESTING] Test dataset \\t Top 1 accuracy: {:.2f}% \\t Top 5 accuracy: {:.2f}%\".format(\n",
    "        top_1_accuracy*100., top_5_accuracy*100.\n",
    "    ))\n",
    "    \n",
    "    return top_1_accuracy, top_5_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions, optimizer, scheduler\n",
    "\n",
    "epochs = 20\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "train_loss_hist, train_acc1_hist, train_acc5_hist = [], [], []\n",
    "val_loss_hist, val_acc1_hist, val_acc5_hist = [], [], []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    train_loss, train_acc1, train_acc5 = train(model, dataloaders['train'], optimizer, loss_function, epoch)\n",
    "    val_loss, val_acc1, val_acc5 = validate(model, dataloaders['valid'], loss_function, epoch)\n",
    "    \n",
    "    train_loss_hist.append(train_loss)\n",
    "    train_acc1_hist.append(train_acc1)\n",
    "    train_acc5_hist.append(train_acc5)\n",
    "    val_loss_hist.append(val_loss)\n",
    "    val_acc1_hist.append(val_acc1)\n",
    "    val_acc5_hist.append(val_acc5)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "# Test model on test data\n",
    "\n",
    "test(model, dataloaders['test'], loss_function)\n",
    "\n",
    "\n",
    "# Plot losses and accuracies\n",
    "# Train\n",
    "fig = plt.figure(figsize=(18, 4))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax1.plot(train_loss_hist, color='blue', alpha=0.7, label='Train')\n",
    "ax1.plot(val_loss_hist, color='orange', alpha=0.8, label='Validation')\n",
    "ax1.set_title('Loss', fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Nonlinear logloss')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax2.plot(train_acc1_hist, color='blue', alpha=0.7, label='Train')\n",
    "ax2.plot(val_acc1_hist, color='orange', alpha=0.8, label='Validation')\n",
    "ax2.set_title('Top 1 accuracy', fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid()\n",
    "\n",
    "ax3 = fig.add_subplot(133)\n",
    "ax3.plot(train_acc5_hist, color='blue', alpha=0.7, label='Train')\n",
    "ax3.plot(val_acc5_hist, color='orange', alpha=0.8, label='Validation')\n",
    "ax3.set_title('Top 5 accuracy', fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.legend()\n",
    "ax3.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
